<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/">
  <channel>
    <title>Author: Matthew Weier O'Phinney :: phly, boy, phly</title>
    <description>Author: Matthew Weier O'Phinney :: phly, boy, phly</description>
    <pubDate>Mon, 25 Feb 2013 12:29:00 +0000</pubDate>
    <generator>Zend_Feed_Writer 2 (http://framework.zend.com)</generator>
    <link>http://mwop.net/blog/author/matthew.html</link>
    <atom:link rel="self" type="application/rss+xml" href="http://mwop.net/blog/author/matthew-rss.xml"/>
    <item>
      <title>RESTful APIs with ZF2, Part 3</title>
      <pubDate>Mon, 25 Feb 2013 12:29:00 +0000</pubDate>
      <link>http://mwop.net/blog/2013-02-25-restful-apis-with-zf2-part-3.html</link>
      <guid>http://mwop.net/blog/2013-02-25-restful-apis-with-zf2-part-3.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    In my <a href="/blog/2013-02-11-restful-apis-with-zf2-part-1.html">previous</a> 
    <a href="/blog/2013-02-13-restful-apis-with-zf2-part-2.html">posts</a>, I 
    covered basics of JSON hypermedia APIs using Hypermedia Application Language
    (HAL), and methods for reporting errors, including API-Problem and vnd.error.
</p>

<p>
    In this post, I'll be covering <em>documenting</em> your API -- techniques 
    you can use to indicate what HTTP operations are allowed, as well as convey 
    the full documentation on what endpoints are available, what they accept, 
    and what you can expect them to return.
</p>

<p>
    While I will continue covering general aspects of RESTful APIs in this 
    post, I will also finally introduce several ZF2-specific techniques.
</p><h2>Why Document?</h2>

<p>
    If you're asking this question, you've either never consumed software, or
    your software is perfect and self-documenting. I frankly don't believe 
    either one.
</p>

<p>
    In the case of APIs, those consuming the API need to know how to use it. 
</p>

<ul>
    <li>What endpoints are available? Which operations are available for each endpoint?</li>
    <li>What does each endpoint expect as a payload during the request?</li>
    <li>What can you expect as a payload in return?</li>
    <li>How will errors be communicated?</li>
</ul>

<p>
    While the promise of hypermedia APIs is that each response tells you the
    next steps available, you still, somewhere along the way, need more
    information - what payloads look like, which HTTP verbs should be used,
    and more. If you're <strong>not</strong> documenting your API, you're
    "doing it wrong."
</p>

<h2>Where Should Documentation Live?</h2>

<p>
    This is the much bigger question.
</p>

<p>
    Of the questions I raised above, detailing what should be documented, there
    are two specific types. When discussing what operations are available, 
    we have a technical solution in the form of the <code>OPTIONS</code>
    method and its counterpart, the <code>Allow</code> header. Everything
    else falls under end-user documentation.
</p>

<h2>OPTIONS</h2>

<p>
    The HTTP specification details the <code>OPTIONS</code> method as 
    idempotent, non-cacheable, and for use in detailing what operations
    are available for the given resource specified by the request URI. It
    makes specific mention of the <code>Allow</code> header, but does not
    limit what is returned for requests made via this method.
</p>

<p>
    The <code>Allow</code> header details the allowed HTTP methods for the
    given resource.
</p>

<p>
    Used in combination, you make an <code>OPTIONS</code> request to a URI,
    and it should return a response containing an <code>Allow</code> header;
    from that header value, you then know what other HTTP methods can be made
    to that URI.
</p>

<p>
    What this tells us is that our RESTful endpoint should do the following:
</p>

<ul>
    <li>
        When an <code>OPTIONS</code> request is made, return a response with
        an <code>Allow</code> header that has a list of the available HTTP
        methods allowed.
    </li>

    <li>
        For any HTTP method we do <em>not</em> allow, we should return a
        "405 Not Allowed" response.
    </li>
</ul>

<p>
    These are fairly easy to accomplish in ZF2. <em>(See? I promised I'd
    get to some ZF2 code in this post!)</em>
</p>

<p>
    When creating RESTful endpoints in ZF2, I recommend using
    <code>Zend\Mvc\Controller\AbstractRestfulController</code>. This controller
    contains an <code>options()</code> method which you can use to respond to
    an <code>OPTIONS</code> request. As with any ZF2 controller, returning
    a response object will prevent rendering and bubble out immediately so
    that the response is returned.
</p>

<div class="example"><pre><code language="php">
namespace My\Controller;
use Zend\Mvc\Controller\AbstractRestfulController;

class FooController extends AbstractRestfulController
{
    public function options()
    {
        $response = $this->getResponse();
        $headers  = $response->getHeaders();

        // If you want to vary based on whether this is a collection or an
        // individual item in that collection, check if an identifier from
        // the route is present
        if ($this->params()->fromRoute('id', false)) {
            // Allow viewing, partial updating, replacement, and deletion
            // on individual items
            $headers->addHeaderLine('Allow', implode(',', array(
                'GET',
                'PATCH',
                'PUT',
                'DELETE',
            )));
            return $response;
        }

        // Allow only retrieval and creation on collections
        $headers->addHeaderLine('Allow', implode(',', array(
            'GET',
            'POST',
        )));
        return $response;
    }
}
</code></pre></div>

<p>
    The next trick is returning the 405 response if an invalid option is used.
    For this, you can create a listener in your controller, and wire it to 
    listen at higher-than-default priority. As an example:
</p>

<div class="example"><pre><code language="php">
namespace My\Controller;
use Zend\EventManager\EventManagerInterface;
use Zend\Mvc\Controller\AbstractRestfulController;

class FooController extends AbstractRestfulController
{
    protected $allowedCollectionMethods = array(
        'GET',
        'POST',
    );

    protected $allowedResourceMethods = array(
        'GET',
        'PATCH',
        'PUT',
        'DELETE',
    );

    public function setEventManager(EventManagerInterface $events)
    {
        parent::setEventManager($events);
        $events->attach('dispatch', array($this, 'checkOptions'), 10);
    }

    public function checkOptions($e)
    {
        $matches  = $e->getRouteMatch();
        $response = $e->getResponse();
        $request  = $e->getRequest();
        $method   = $request->getMethod();

        // test if we matched an individual resource, and then test
        // if we allow the particular request method
        if ($matches->getParam('id', false)) {
            if (!in_array($method, $this->allowedResourceMethods)) {
                $response->setStatusCode(405);
                return $response;
            }
            return;
        }

        // We matched a collection; test if we allow the particular request 
        // method
        if (!in_array($method, $this->allowedCollectionMethods)) {
            $response->setStatusCode(405);
            return $response;
        }
    }
}
</code></pre></div>

<p>
    Note that I moved the allowed methods into properties; if I did the above,
    I'd refactor the <code>options()</code> method to use those properties as
    well to ensure they are kept in sync.
</p>

<p>
    Also note that in the case of an invalid method, I return a response object.
    This ensures that nothing else needs to execute in the controller; I
    discover the problem and return early.
</p>

<h2>End-User Documentation</h2>

<p>
    Now that we have the technical solution out of the way, we're still left 
    with the bulk of the work left to accomplish: providing end-user 
    documentation detailing the various payloads, errors, etc.
</p>

<p>
    I've seen two compelling approaches to this problem. The first builds on
    the <code>OPTIONS</code> method, and the other uses a hypermedia link in
    every response to point to documentation.
</p>

<p>
    The <code>OPTIONS</code> solution is this: <a 
    href="http://zacstewart.com/2012/04/14/http-options-method.html">use the 
    body of an <code>OPTIONS</code> response to provide documentation</a>.
    (Keith Casey <a href="http://vimeo.com/49613738">gave an excellent short 
    presentation about this at REST Fest 2012</a>).
</p>

<p>
    The <code>OPTIONS</code> method allows for you to return a body in the
    response, and also allows for content negotiation. The theory, then, is
    that you return media-type-specific documentation that details the
    methods allowed, and what they specifically accept in the body. While
    there is no standard for this at this time, the first article I linked
    suggested including a description, the parameters expected, and one or more 
    example request bodies for each HTTP method allowed; you'd likely also
    want to detail the responses that can be expected.
</p>

<div class="example"><pre><code language="javascript">
{
    "POST": {
        "description": "Create a new status",
        "parameters": {
            "type": {
                "type": "string",
                "description": "Status type -- text, image, or url; defaults to text",
                "required": false
            },
            "text": {
                "type": "string",
                "description": "Status text; required for text types, optional for others",
                "required": false
            },
            "image_url": {
                "type": "string",
                "description": "URL of image for image types; required for image types",
                "required": false
            },
            "link_url": {
                "type": "string",
                "description": "URL of image for link types; required for link types",
                "required": false
            }
        },
        "responses": [
            {
                "describedBy": "http://example.com/problems/invalid-status",
                "title": "Submitted status was invalid",
                "detail": "Missing text field required for text type"
            },
            {
                "id": "abcdef123456",
                "type": "text",
                "text": "This is a status update",
                "timestamp": "2013-02-22T10:06:05+0:00"
            }
        ],
        "examples": [
            {
                "text": "This is a status update"
            },
            {
                "type": "image",
                "text": "This is the image caption",
                "image_url": "http://example.com/favicon.ico"
            },
            {
                "type": "link",
                "text": "This is a description of the link",
                "link_url": "http://example.com/"
            },
        ]
    }
}
</code></pre></div>

<p>
    If you were to use this methodology, you would alter the 
    <code>options()</code> method such that it does not return a response
    object, but instead return a view model with the documentation.
</p>

<div class="example"><pre><code language="php">
namespace My\Controller;
use Zend\Mvc\Controller\AbstractRestfulController;

class FooController extends AbstractRestfulController
{
    protected $viewModelMap = array(/* ... */);

    public function options()
    {
        $response = $this->getResponse();
        $headers  = $response->getHeaders();

        // Get a view model based on Accept types
        $model    = $this->acceptableViewModelSelector($this->viewModelMap);

        // If you want to vary based on whether this is a collection or an
        // individual item in that collection, check if an identifier from
        // the route is present
        if ($this->params()->fromRoute('id', false)) {
            // Still set the Allow header
            $headers->addHeaderLine('Allow', implode(
                ',', 
                $this->allowedResourceMethods
            ));

            // Set documentation specification as variables
            $model->setVariables($this->getResourceDocumentationSpec());
            return $model;
        }

        // Allow only retrieval and creation on collections
        $headers->addHeaderLine('Allow', implode(
            ',',
            $this->allowedCollectionMethods
        ));
        $model->setVariables($this->getCollectionDocumentationSpec());
        return $model;
    }
}
</code></pre></div>

<p>
    I purposely didn't provide the implementations of the 
    <code>getResourceDocumentationSpec()</code> and 
    <code>getCollectionDocumentationSpec()</code> methods, as that will likely
    be highly specific to your application. Another possibility is to use
    your view engine for this, and specify a template file that has the
    fully-populated information. This would require a custom renderer when
    using JSON or XML, but is a pretty easy solution.
</p>

<p>
    <strong>However, there's one cautionary tale to tell</strong>, something I 
    already mentioned: <code>OPTIONS</code>, per the specification, is 
    <em>non-cacheable</em>.  What this means is that everytime somebody makes an 
    <code>OPTIONS</code> request, any cache control headers you provide will be 
    ignored, which means hitting the server for each and every request to the 
    documentation.  Considering documentation is static, this is problematic; 
    it has even prompted <a href="http://www.mnot.net/blog/2012/10/29/NO_OPTIONS">blog 
    posts urging you not to use OPTIONS for documentation</a>.
</p>

<p>
    Which brings us to the second solution for end-user documentation: a static
    page referenced via a hypermedia link.
</p>

<p>
    This solution is insanely easy: you simply provide a <code>Link</code>
    header in your response, and provide a <code>describedby</code> reference
    pointing to the documentation page:
</p>

<div class="example"><pre><code language="http">
Link: &lt;http://example.com/api/documentation.md&gt;; rel="describedby"
</code></pre></div>

<p>
    With ZF2, this is trivially easy to accomplish: create a route and endpoint
    for your documentation, and then a listener on your controller that adds
    the <code>Link</code> header to your response.
</p>

<p>
    The latter, adding the link header, might look like this:
</p>

<div class="example"><pre><code language="php">
namespace My\Controller;
use Zend\EventManager\EventManagerInterface;
use Zend\Mvc\Controller\AbstractRestfulController;

class FooController extends AbstractRestfulController
{
    public function setEventManager(EventManagerInterface $events)
    {
        parent::setEventManager($events);
        $events->attach('dispatch', array($this, 'injectLinkHeader'), 20);
    }

    public function injectLinkHeader($e)
    {
        $response = $e->getResponse();
        $headers  = $response->getHeaders();
        $headers->addHeaderLine('Link', sprintf(
            '<%s>; rel="describedby"', 
            $this->url('documentation-route-name')
        ));
    }
}
</code></pre></div>

<p>
    If you want to ensure you get a fully qualified URL that includes the 
    schema, hostname, and port, there are a number of ways to do that as
    well; the above gives you the basic idea.
</p>

<p>
    Now, for the route and endpoint, there are tools that will help you
    simplify that task as well, in the form of a couple of ZF2 modules:
    <a href="https://github.com/weierophinney/PhlySimplePage">PhlySimplePage</a>
    and <a href="https://github.com/Soflomo/Prototype">Soflomo\Prototype</a>.
    <em>(Disclosure: I'm the author of PhlySimplePage.)</em>
</p>

<p>
    Both essentially allow you to specify a route and the corresponding
    template name to use, which means all you need to do is provide a little
    configuration, and a view template. <code>Soflomo\Prototype</code> has
    slightly simpler configuration, so I'll demonstrate it here:
</p>

<div class="example"><pre><code language="php">
return array(
    'soflomo_prototype' => array(
        'documentation-route-name' => array(
            'route'    => '/api/documentation',
            'template' => 'api/documentation',
        ),
    ),
    'view_manager' => array(
        'template_map' => array(
            'api/documentation' => __DIR__ . '/../view/api/documentation.phtml',
        ),
    ),
);
</code></pre></div>

<p>
    I personally have been using the <code>Link</code> header solution, as it's
    so simple to implement. It does <em>not</em> write the documentation for you,
    but thinking about it early and implementing it helps ensure you at least
    start writing the documentation, and, if you open source your project,
    you may find you have users who will write the documentation for you if
    they know where it lives.
</p>

<h2>Conclusions</h2>

<p>
    Document your API, or either nobody will use it, or all you're hear are
    complaints from your users about having to guess constantly about how to
    use it. Include the following information:
</p>

<ul>
    <li>What endpoint(s) is (are) available.</li>
    <li>Which operations are available for each endpoint.
        <ul>
            <li>What payloads are expected by the endpoint.</li>
            <li>What payloads can a user expect in return.</li>
            <li>What media types may be used for requests.</li>
            <li>What media types may be expected in responses.</li>
        </ul>
    </li>
</ul>

<p>
    Additionally, make sure that you do the <code>OPTIONS</code>/<code>Allow</code>
    dance; don't just accept any request method, and report the standard
    405 response for methods that you will not allow. Make sure you differentiate
    these for collections versus individual resources, as you likely may
    allow replacing or updating an individual resource, but likely will not
    want to do the same for a whole collection!
</p>

<h2>Next time</h2>

<p>
    So far, I've covered the basics of RESTful JSON APIS, specifically 
    recommending Hypermedia Application Language (HAL) for providing hypermedia
    linking and relations. I've covered error reporting, and provided two
    potential formats (API-Problem and vnd.error) for use with your APIs.
    Now, in this article, I've shown a bit about documenting your API both
    for machine consumption as well as end-users. What's left?
</p>

<p>
    In upcoming parts, I'll talk about ZF2's <code>AbstractRestfulController</code>
    in more detail, as well as how to perform some basic content negotiation.
    I've also had requests about how one might deal with API versioning, and will
    attempt to demonstrate some techniques for doing that as well. Finally,
    expect to see a post showing how I've tied all of this together in a 
    general-purpose ZF2 module so that you can ignore all of these posts and simply
    start writing APIs.
</p>

<h3>Updates</h3>

<p>
    <em>Note: I'll update this post with links to the other posts in the series 
    as I publish them.</em>
</p>

<ul>
    <li><a href="/blog/2013-02-11-restful-apis-with-zf2-part-1">Part 1</a></li>
    <li><a href="/blog/2013-02-13-restful-apis-with-zf2-part-2">Part 2</a></li>
</ul>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>RESTful APIs with ZF2, Part 2</title>
      <pubDate>Wed, 13 Feb 2013 13:40:00 +0000</pubDate>
      <link>http://mwop.net/blog/2013-02-13-restful-apis-with-zf2-part-2.html</link>
      <guid>http://mwop.net/blog/2013-02-13-restful-apis-with-zf2-part-2.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    In my <a href="/blog/2013-02-11-restful-apis-with-zf2-part-1.html">last post</a>,
    I covered some background on REST and the Richardson Maturity Model, and some
    emerging standards around hypermedia APIs in JSON; in particular, I outlined
    aspects of Hypermedia Application Language (HAL), and how it can be used to
    define a generic structure for JSON resources.
</p>

<p>
    In this post, I cover an aspect of RESTful APIs that's often overlooked:
    reporting problems.
</p><h2>Background</h2>

<p>
    APIs are useful when they're working. But when they fail, they're only 
    useful if they provide us with meaningful information; if all I get is
    a status code, and no indication of what caused the issue, or where I might
    look for more information, I get frustrated.
</p>

<p>
    In consuming APIs, I've come to the following conclusions:
</p>

<ul>
    <li>
        Error conditions need to provide detailed information as to what went 
        wrong, and what steps I may be able to take next. An error code with
        no context gives me nothing to go on.
    </li>

    <li>
        Errors need to be reported consistently. Don't report the error one way
        one time, and another way the next.
    </li>

    <li>
        <strong>DO</strong> use HTTP status codes to indicate an error happened.
        Nothing is more irksome than getting back a 200 status with an error 
        payload.
    </li>

    <li>
        Errors should be reported in a format I have indicated I will Accept 
        (as in the HTTP header). Perhaps the only think more irksome than a 200
        status code for an error is getting back an HTML page when I expect 
        JSON.
    </li>
</ul>

<h2>Why Status Codes Aren't Enough</h2>

<p>
    Since REST leverages and builds on HTTP, an expedient solution for reporting
    problems is to simply use <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP status codes</a>. 
    These are well understood by web developers, right?
</p>

<p>
    <code>4xx</code> error codes are errors made by the requestor, and are actually fairly 
    reasonable to use for reporting things such as lack of authorization tokens,
    incomplete requests, unsupportable operations, or non-supported media types.
</p>

<p>
    But what happens when the error is on the server - because something has
    gone wrong such as inability to reach your persistence layer or credential
    storage? The <code>5xx</code> series of status codes is sparse and wholly unsuited to
    reporting errors of these types -- <em>though you'll likely still want to use
    a <code>500</code> status to report the failure</em>. But what do you present to the consumer
    so that they know whether or not to try again, or what to report to you
    so that you can fix the issue?
</p>

<p>
    A status code simply isn't enough information most of the time. Yes, you 
    want to define standard status codes so that your clients can perform
    reasonable branching, but you also need a way to communicate <em>details</em>
    to the end-user, so that they can log the information for themselves, display
    information to their own end-users, and/or report it back to you so you can
    do something to resolve the situation.
</p>

<h2>Custom Media Types</h2>

<p>
    The first step is to use a custom media type. Media types are typically 
    both a name as well as a structure -- and the latter is what we're after
    when it comes to error reporting. 
</p>

<p>
    If we return a response using this media type, the client then knows how
    to parse it, and can then process it, log it, whatever.
</p>

<p>
    Sure, you can make up your own format -- as long as you are consistent
    in using it, and you document it. But personally, I don't like inventing
    new formats when standard formats exist already. Custom formats mean that 
    custom clients are required for working with the services; using a standard
    format can save effort and time.
</p>

<p>
    In the world of JSON, I've come across two error media types that appear to
    be gaining traction: <code>application/api-problem+json</code> and 
    <code>application/vnd.error+json</code>
</p>

<h3>API-Problem</h3>

<p>
    This particular media type is <a 
    href="http://tools.ietf.org/html/draft-nottingham-http-problem-02">via the 
    IETF</a>. Like HAL, it provides formats in both JSON and XML, making it 
    a nice cross-platform choice.
</p>

<p>
    As noted already, the media type is <code>application/api-problem+json</code>.
    The representation is a single resource, with the following properties:
</p>

<ul>
    <li>
        <strong>describedBy</strong>: a URL to a document describing the error condition (required)
    </li>

    <li>
        <strong>title</strong>: a brief title for the error condition (required)
    </li>

    <li>
        <strong>httpStatus</strong>: the HTTP status code for the current request (optional)
    </li>

    <li>
        <strong>detail</strong>: error details specific to this request (optional)
    </li>

    <li>
        <strong>supportId</strong>: a URL to the specific problem occurrence (e.g., to a log message) (optional)
    </li>
</ul>

<p>
    As an example:
</p>

<div class="example"><pre><code language="http">
HTTP/1.1 500 Internal Error
Content-Type: application/api-problem+json

{
    "describedBy": "http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html",
    "detail": "Status failed validation",
    "httpStatus": 500,
    "title": "Internal Server Error"
}
</code></pre></div>

<p>
    The specification allows a large amount of flexibility -- you can have your 
    own custom error types, so long as you have a description of them to link 
    to. You can provide as little or as much detail as you want, and even 
    decide what information to expose based on environment.
</p>

<p>
    I personally like to point to the HTTP status code definitions, and then 
    provide request-specific detail; I find this gives quick and simple results 
    that I can later shape as I add more detail to my API. However, the 
    specification definitely encourages you to have unique error types with 
    discrete URIs that describe them -- never a bad thing when creating APIs.
</p>

<h3>vnd.error</h3>

<p>
    This is a <a href="https://github.com/blongden/vnd.error">proposed media 
    type</a> within the HAL community. Like HAL, it provides formats in both 
    JSON and XML, making it a nice cross-platform choice.
</p>

<p>
    It differentiates from API-Problem in a few ways. First, it allows, and 
    even encourages, reporting collections of errors. If you consider PHP 
    exceptions and the fact that they support "previous" exceptions, this is a 
    powerful concept; you can report the entire chain of errors that led to the 
    response.  Second, it encourages pushing detail out of the web service; 
    errors include a "logRef" property that points to where the error detail lives. 
    This is probably better illustrated than explained.
</p>

<p>
    The response payload is an array of objects. Each object has the following 
    members:
</p>

<ul>
    <li>
        <strong>logRef</strong>: a unique identifier for the specific error which can then be
        used to identify the error within server-side logs (required)
    </li>

    <li>
        <strong>message</strong>: the error message itself (required)
    </li>

    <li>
        <strong>_links</strong>: HAL-compatible links. Typically, "help", "describes", and/or 
        "describedBy" relations will be defined here.
    </li>
</ul>

<p>
    As an example, let's consider the API-Problem example I had earlier, and
    provide a vnd.error equivalent:
</p>

<div class="example"><pre><code language="http">
HTTP/1.1 500 Internal Error
Content-Type: application/vnd.error+json

[
    {
        "logRef": "someSha1HashMostLikely",
        "message": "Status failed validation",
        "_links": {
            "describedBy": {"href": "http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"}
        }
    }
]
</code></pre></div>

<p>
    vnd.error basically begs you to create custom error types, with documentation 
    end-points that detail the source of the error and what you can do about it 
    (this is true of API-Problem as well).
</p>

<p>
    The requirement to include a log reference ("logRef") and have it be unique 
    can be a stumbling block to implementation, however, as it requires effort 
    for uniquely identifying requests, and logging. However, both the 
    identification and logging can be automated.
</p>

<h2>Summary</h2>

<p>
    Error reporting in APIs is as important as the normal resource payloads 
    themselves. Without good error reporting, when an API raises errors, 
    clients have difficulty understanding what they can do next, and cannot
    provide you, the API provider, with information that will allow you to
    debug on the server side.
</p>

<p>
    As noted at the beginning of the article, if you follow the rules below,
    you'll make consumers of your API happier and more productive.
</p>

<ul>
    <li>
        <strong>DO</strong> use appropriate HTTP status codes to indicate an 
        error happened.
    </li>

    <li>
        Report errors in a format I have indicated I will Accept 
        (as in the HTTP header). 
    </li>
    <li>
        Report errors consistently. Don't report the error one way one time, 
        and another way the next. Standardize on a specific error-reporting 
        media type .  While you <em>can</em> create your own error structure, I 
        recommend using documented, accepted standards. This will make clients 
        more re-usable, and make many of your decisions for you.
    </li>

    <li>
        Provide detailed information as to what went wrong, and what steps I 
        may be able to take next. Provide documentation for each type of error, 
        and link to that documentation from your error payloads.
    </li>

</ul>

<p>
    Which brings me to...
</p>

<h2>Next time</h2>

<p>
    I realize I still haven't covered anything specific to ZF2, but I'll start 
    next time, when I cover the next topic: documenting your API. An 
    undocumented API is a useless API, so it's good to start baking 
    documentation in immediately. I'll survey some of the possibilities and how 
    they can be implemented in ZF2 in the next installment, and then we can get 
    our hands dirty with actual API development.
</p>

<h3>Updates</h3>

<p>
    <em>Note: I'll update this post with links to the other posts in the series 
    as I publish them.</em>
</p>

<ul>
    <li><a href="/blog/2013-02-11-restful-apis-with-zf2-part-1">Part 1</a></li>
    <li><a href="/blog/2013-02-25-restful-apis-with-zf2-part-3">Part 3</a></li>
</ul>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>RESTful APIs with ZF2, Part 1</title>
      <pubDate>Wed, 13 Feb 2013 13:40:00 +0000</pubDate>
      <link>http://mwop.net/blog/2013-02-11-restful-apis-with-zf2-part-1.html</link>
      <guid>http://mwop.net/blog/2013-02-11-restful-apis-with-zf2-part-1.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    RESTful APIs have been an interest of mine for a couple of years, but due
    to <a href="http://framework.zend.com/blog//zend-framework-2-0-0-stable-released.html">circumstances</a>,
    I've not had much chance to work with them in any meaningful fashion until
    recently.
</p>

<p>
    <a href="http://akrabat.com/">Rob Allen</a> and I proposed a workshop for
    <a href="http://conference.phpbenelux.eu/2013/">PHP Benelux 2013</a> 
    covering RESTful APIs with ZF2. When it was accepted, it gave me the perfect
    opportunity to dive in and start putting the various pieces together.
</p><h2>Background</h2>

<p>
    I've attended any number of conference sessions on API design, read 
    countless articles, and engaged in quite a number of conversations. Three
    facts keep cropping up:
</p>

<ol>
    <li>JSON is fast becoming the preferred exchange format due to the ease 
    with which it de/serializes in almost every language.</li>
    <li>The "holy grail" is <a 
    href="http://martinfowler.com/articles/richardsonMaturityModel.html">Richardson 
    Maturity Model</a> Level 3.</li>
    <li>It's really hard to achieve RMM level 3 with JSON.</li>
</ol>

<h3>Richardson Maturity Model</h3>

<p>
    As a quick review, the Richardson Maturity Model has the following 4 levels:
</p>

<ul>
    <li>Level 0: "The swamp of POX." Basically, a service that uses TCP for 
        transport, primarily as a form of remote procedure call (RPC). 
        Typically, these are not really leveraging HTTP in any meaningful 
        fashion; most systems will use HTTP POST for all interactions. Also, 
        you will often have a single endpoint for all interactions, regardless 
        of whether or not they are strictly related. XML-RPC, SOAP, and 
        JSON-RPC fall under this category.
    </li>

    <li>Level 1: "Resources." In these services, you start breaking the service
        into multiple services, one per "resource," or, in object oriented terms,
        per object. This means a distinct URL per object, which means each
        has its own distinct identity on the web; this often extends not only 
        to the collection of objects, but to individual objects under the 
        collection as well (e.g., "/books" as well as "/books/life-of-pi"). The 
        service may still be RPC in nature, however, and, at this level, often 
        is still using a single HTTP method for all interactions with the 
        resource.
    </li>

    <li>Level 2: "HTTP Verbs." At this level, we start using HTTP verbs with
        our services in the way the HTTP specification intends. GET is for safe 
        operations, and should be cacheable; POST is used for creation and/or 
        updating; DELETE can be used to delete a resource; etc. Rather than 
        doing RPC style methods, we leverage HTTP, occasionally passing 
        additional parameters via the query string or request body.  
        Considerations such as HTTP caching and idempotence are taken into 
        account.
    </li>

    <li>Level 3: "Hypermedia Controls." Building on the previous level, our
        resource representations now also include <em>links</em>, which indicate
        what we can <em>do next</em>. At this level, our API becomes practically
        self-describing; given a single end-point, we should be able to start
        crawling it, using the links in a representation to lead us to the next
        actions.
    </li>
</ul>

<p>
    When I first started playing with web services around a decade ago, 
    everything was stuck at Level 0 or Level 1 -- usually with Level 1 users 
    downgrading to Level 0 because Level 0 offerred consistency and 
    predictability if you chose to use a service type that had a defined 
    envelope format (such as XML-RPC or SOAP).  (I even wrote the XML-RPC 
    server implementation for Zend Framework because I got sick of writing 
    one-off parsers/serializers for custom XML web service implementations.
    When you're implementing many services, predictability is a huge win.)
</p>

<p>
    A few years ago, I started seeing a trend towards Level 2. Web developers
    like the simplicity of using HTTP verbs, as they map very well to <a 
    href="http://en.wikipedia.org/wiki/Create,_read,_update_and_delete">CRUD</a>
    operations -- the bread and butter of web development. Couple this concept
    with JSON, and it becomes trivially simple to both create a web service, as
    well as consume it.
</p>

<p>
    <em>I'd argue that the majority of web developers are quite happy to be at 
    Level 2 -- and have no problem staying there. They're productive, and the 
    concepts are easy -- both to understand and to implement.</em>
</p>

<p>
    Level 3, though, is where it becomes really interesting. The idea that
    I can examine the represention <em>alone</em> in order to understand what
    I can do next is very intriguing and empowering.
</p>

<h3>JSON and Hypermedia</h3>

<p>
    With XML, hypermedia basically comes for free. Add some &lt;link&gt; 
    elements to your representation, and you're done -- and don't forget the 
    link <code>rel</code>ations!
</p>

<p>
    JSON, however, is another story.
</p>

<p>
    Where do the links go? <em>There is no single, defined way to represent a 
    hyperlink in JSON.</em>
</p>

<p>
    Fortunately, there are some emerging standards. 
</p>

<p>
    First is use of the <a href="http://www.w3.org/wiki/LinkHeader">"Link" HTTP 
    header</a>. While the page I linked shows only a single link in the
    header, you can have multiple links separated by commas. GitHub uses this
    when providing pagination links in their API. Critics will point out that
    the HTTP headers are not technically part of the representation, however;
    strict interpetations of REST and RMM indicate that the hypermedia links
    should be part of the resource representation. Regardless, having the links
    in the HTTP headers is useful for pre-traversal of a service, as you can
    perform HEAD requests only to discover possible actions and workflows.
</p>

<p>
    <a 
    href="http://amundsen.com/media-types/collection/format/">Collection+JSON</a>
    is interesting, as it describes the entire JSON envelope. My one criticism
    is that it details too much; whenever I see a format that dictates how to 
    describe types, I think of XML-RPC or SOAP, and get a little twitchy. It's
    definitely worth a look, though.
</p>

<p>
    What's captured my attention of late, however, is 
    <a href="http://stateless.co/hal_specification.html">Hypertext Application Language</a>,
    or HAL for short. HAL has very few rules, but succinctly describes both how
    to provide hypermedia in JSON as well as how to represent embedded resources
    - the two things that most need standardized structure in JSON. It does this
    while still providing a generic media type, and also describing a mirror 
    image XML format!
</p>

<h3>HAL Media Types</h3>

<p>
    HAL defines two generic media types: <code>application/hal+xml</code> and
    <code>application/hal+json</code>. You will use these as the response
    Content-Type, as they describe the response representation; the client
    can simply request <code>application/json</code>, and the response
    format remains compatible.
</p>

<h3>HAL and Links</h3>

<p>
    HAL provides a very simple structure for JSON hypermedia links. First, all 
    resource representations must contain hypermedia links, and all links are 
    provided in a "_links" object:
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
    }
}
</code></pre></div>

<p>
    Second, links are properties of this object. The property name is the link
    relation, and the value is an object containing minimally an "href" 
    property.
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
        "self": {"href": "http://example.com/api/status/1234"}
    }
}
</code></pre></div>

<p>
    If a given relation can have multiple links, you provide instead an array
    of objects:
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
        "self": {"href": "http://example.com/api/status/1234"},
        "conversation": [
            {"href": "http://example.com/api/status/1237"},
            {"href": "http://example.com/api/status/1241"}
        ]
    }
}
</code></pre></div>

<p>
    Individual links can contain other attributes as desired -- I've seen
    people include the relation again so that it's self-contained in the link 
    object, and it's not uncommon to include a title or name.
</p>

<h3>HAL and Resources</h3>

<p>
    HAL imposes no structure over resources other than requiring the hypermedia 
    links; even then, you typically do not include the hypermedia links when 
    making a request of the web service; the hypermedia links are included only 
    in the representations <em>returned</em> by the service.
</p>

<p>
    So, as an example, you would POST the following:
</p>

<div class="example"><pre><code language="http">
POST /api/status
Host: example.com
Accept: application/json
Content-Type: application/json

{
    "status": "This is my awesome status update!",
    "user": "mwop"
}
</code></pre></div>

<p>
    And from that request, you'd receive the following:
</p>

<div class="example"><pre><code language="http">
201 Created
Location: http://example.com/api/status/1347
Content-Type: application/hal+json

{
    "_links": {
        "self": {"href": "http://example.com/api/status/1347"}
    },
    "id": "1347",
    "timestamp": "2013-02-11 23:33:47",
    "status": "This is my awesome status update!",
    "user": "mwop"
}
</code></pre></div>

<h3>HAL and Embedded Resources</h3>

<p>
    The other important thing that HAL defines is how to <em>embed</em>
    resources. Why is this important? If the resource references other
    resources, you will want to be able to link to them so you can perform
    operations on them, too.
</p>

<p>
    Embedded resources are represented inside an "_embedded" object of the
    representation, and, as resources, contain their own "_links" object as 
    well. Each resource you embed is assigned to a property of that
    object, and if multiple objects of the same type are returned, an array
    of resources is assigned. In fact, this latter is how you represent
    <em>collections</em> in HAL.
</p>

<p>
    Let's consider a simple example first. In previous code samples, I have
    a "user" that's a string; let's make that an embedded resource instead.
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
        "self": {"href": "http://example.com/api/status/1347"}
    },
    "id": "1347",
    "timestamp": "2013-02-11 23:33:47",
    "status": "This is my awesome status update!",
    "_embedded": {
        "user": {
            "_links": {
                "self": {"href": "http://example.com/api/user/mwop"}
            }
            "id": "mwop",
            "name": "Matthew Weier O'Phinney",
            "url": "http://mwop.net"
        }
    }
}
</code></pre></div>

<p>
    I've moved the "user" out of the representation, and into the "_embedded"
    object -- because this is where you define embedded resources. Note that
    the "user" is a standard HAL resource itself -- containing hypermedia links.
</p>

<p>
    Now let's look at a collection:
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
        "self": {"href": "http://example.com/api/status"},
        "next": {"href": "http://example.com/api/status?page=2"},
        "last": {"href": "http://example.com/api/status?page=100"}
    },
    "count": 2973,
    "per_page": 30,
    "page": 1,
    "_embedded": {
        "status": [
            {
                "_links": {
                    "self": {"href": "http://example.com/api/status/1347"}
                },
                "id": "1347",
                "timestamp": "2013-02-11 23:33:47",
                "status": "This is my awesome status update!",
                "_embedded": {
                    "user": {
                        "_links": {
                            "self": {"href": "http://example.com/api/user/mwop"}
                        }
                        "id": "mwop",
                        "name": "Matthew Weier O'Phinney",
                        "url": "http://mwop.net"
                    }
                }
            }
            /* ... */
        ]
    }
}
</code></pre></div>

<p>
    Note that the "status" property is an array; semantically, all resources
    under this key are of the same type. Also note that the parent resource
    has some additional link relations -- these are related to pagination, and
    allow a client to determine what the next and last pages are (and, if we 
    were midway into the collection, previous and first pages). Since the 
    collection is also a resource, it has some interesting metadata -- how
    many resources are in the collection, how many we represent per page, and
    what the current page is.
</p>

<p>
    Also note that you can nest resources -- simply include an "_embedded" 
    object inside an embedded resource, with additional resources, as I've
    done with the "user" resource inside the status resource shown here. It's 
    turtles all the way down.
</p>

<h2>Next Time</h2>

<p>
    The title of this post indicates I'll be talking about building RESTful 
    APIs with ZF2 -- but so far, I've not said anything about ZF2.
</p>

<p>
    I'll get there. But there's another detour to take: reporting errors.
</p>

<h3>Updates</h3>

<p>
    <em>Note: I'll update this post with links to the other posts in the series 
    as I publish them.</em>
</p>

<ul>
    <li><a href="/blog/2013-02-13-restful-apis-with-zf2-part-2">Part 2</a></li>
    <li><a href="/blog/2013-02-25-restful-apis-with-zf2-part-3">Part 3</a></li>
</ul>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>OpenShift, Cron, and Naked Domains</title>
      <pubDate>Sun, 30 Dec 2012 15:52:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-12-30-openshift-cron-and-naked-domains.html</link>
      <guid>http://mwop.net/blog/2012-12-30-openshift-cron-and-naked-domains.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    As an experiment, I migrated my website over to <a 
    href="http://openshift.redhat.com">OpenShift</a> yesterday. I've been hosting
    a pastebin there already, and have found the service to be both straightforward
    and flexible; it was time to put it to a more thorough test.
</p>

<p>
    In the process, I ran into a number of interesting issues, some of which took quite
    some time to resolve; this post is both to help inform other potential users of the
    service, as well as act as a reminder to myself.
</p><h2>Cron</h2>

<p>
    OpenShift offers a <a href="http://en.wikipedia.org/wiki/Cron">Cron</a> cartridge,
    which I was excited to try out.<sup><a href="#f1">1</a></sup>
</p>

<p>
    The basics are quite easy. In your repository's <code>.openshift</code> 
    directory is a <code>cron</code> subdirectory, further divided into 
    <code>minutely</code>, <code>hourly</code>, <code>daily</code>, <code>weekly</code>,
    and <code>monthly</code> subdirectories. You drop a script you want to run
    into one of these directories, and push your changes upstream.
</p>

<p>
    The problem is: what if I want a job to run at a specific time daily? or on 
    the quarter hour? or on a specific day of the week?
</p>

<p>
    As it turns out, you can manage all of the above, just not quite as succinctly as
    you would in a normal crontab. Here, for example, is a script that I run at 
    5AM daily; I placed it in the <code>hourly</code> directory so that it can test
    more frequently:
</p>

<div class="example"><pre><code language="bash">
#!/bin/bash
if [ `date +%H` == "05" ]
then
    (
        export PHP=/usr/local/zend/bin/php ;
        cd $OPENSHIFT_REPO_DIR ; 
        $PHP public/index.php phlycomic fetch all ; 
        $PHP public/index.php phlysimplepage cache clear --page=pages/comics 
    )
fi
</code></pre></div>

<p>
    And here's one that runs on the quarter-hour, placed in the <code>minutely</code>
    directory:
</p>

<div class="example"><pre><code language="bash">
#!/bin/bash
MINUTES=`date +%M`

for i in "00" "15" "30" "45";do
    if [ "$MINUTES" == "$i" ];then
        (
            export PHP=/usr/local/zend/bin/php ;
            cd $OPENSHIFT_REPO_DIR ;
            $PHP public/index.php githubfeed fetch 
        )
    fi
done
</code></pre></div>

<p>
    The point is that if you need more specificity, push the script into the 
    next more specific directory, and test against the time of execution.
</p>

<h2>Naked Domains</h2>

<p>
    Naked domains are domains without a preceding subdomain. In my case, this
    means "mwop.net", vs. "www.mwop.net".
</p>

<p>
    The problem that cloud hosting presents is that the IP address on which you
    are hosted can change at any time, for a variety of reasons. As such, you
    typically cannot use DNS A records to point to your domain; the recommendation
    is to use a CNAME record that points the domain to a "virtual" domain 
    registered with your cloud hosting provider.
</p>

<p>
    However, most domain registrars and DNS providers do not let you do this for
    a naked domain, particularly if you also have MX or other records associated
    with that naked domain.
</p>

<p>
    Some registrars will allow you to forward the A record to a subdomain. I tried
    this, but had limited success; I personally found that I ended up in an infinite
    loop situation when doing the DNS lookup.
</p>

<p>
    Another solution is to have a redirect in place for your naked domain to the
    subdomain, which can then be a CNAME record. Typically, this would require you
    have a web server under your control with a fixed IP that then simply redirects
    to the subdomain. Fortunately, there's an easier solution: <a 
    href="http://wwwizer.com/naked-domain-redirect">wwwizer</a>. You simply point
    your naked domain A record to the wwwizer IP address, and they will do a 
    redirect to your <code>www</code> subdomain.
</p>

<p>
    I implemented wwwizer on my domain (which is why you'll see "www.mwop.net" in
    your location bar), and it's been working flawlessly since doing so.
</p>

<h4>Private repositories</h4>

<p>
    I keep my critical site settings in a private repository, which allows me 
    to version them while keeping the credentials they hold out of the public eye.
    This means, however, that I need to use <a href="https://help.github.com/articles/managing-deploy-keys">GitHub deploy keys</a> on my server
    in order to retrieve changes.
</p>

<p>
    This was simple enough: I created an <code>ssh</code> subdirectory in my
    <code>$OPENSHIFT_DATA_DIR</code> directory, and generated a new SSH keypair.
</p>

<p>
    The problem was telling SSH to <em>use</em> this key when fetching changes.
</p>

<p>
    The solution is to use a combination of <code>ssh-agent</code> and <code>ssh-add</code>,
    and it looks something like this:
</p>

<div class="example"><pre><code language="bash">
#!/bin/bash
ssh-agent `ssh-add $OPENSHIFT_DATA_DIR/ssh/github-key && (
    cd $OPENSHIFT_DATA_DIR/config ; 
    git fetch origin ; 
    git rebase origin/mwop.net.config
)`
</code></pre></div>

<p>
    After testing the above, I put this in a <code>pre_build</code> script in 
    my OpenShift configuration so that I can autoupdate my private 
    configuration on each build. However, I discovered a new problem: when
    a build is being done, the <code>ssh-agent</code> is not available, which
    means the above cannot be executed. I'm still trying to find a solution.
</p>

<h2>Fin</h2>

<p>
    I'm pretty happy with the move. I don't have to do anything special
    to automate deployment, and all my cronjobs and deployment scripts are now
    self-contained in the repository, which makes my site more portable.
    While a few things could use more documentation, all the pieces are there
    and discoverable with a small amount of work.
</p>

<p>
    I'll likely give some other PaaS providers a try in the future, but for 
    the moment, I'm quite happy with the functionality and flexibility of 
    OpenShift.
</p>

<h4>Footnotes</h4>

<ul>
    <li id="f1">Zend Server's JobQueue can also be used as a cron replacement, 
    but I was not keen on exposing the job functionality via HTTP.</li>
</ul>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>On php-fig and Shared Interfaces</title>
      <pubDate>Thu, 20 Dec 2012 20:23:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-12-20-on-shared-interfaces.html</link>
      <guid>http://mwop.net/blog/2012-12-20-on-shared-interfaces.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    This is a post I've been meaning to write for a long time, and one requested
    of me personally by <a href="http://www.rooftopsolutions.nl/blog/">Evert 
    Pot</a> during the Dutch PHP Conference in June 2012. It details some observations
    I have of php-fig, and hopefully will serve as a record of why I'm not
    directly participating any longer.
</p>

<p>
    I was a founding member of the <a href="http://www.php-fig.org/">Framework 
    Interoperability Group</a>, now called "php-fig". I was one of around a dozen 
    folks who sat around a table in 2009 in Chicago during php|tek and started 
    discussions about what we could all do to make it possible to work better 
    together between our projects, and make it simpler for users to pick and choose 
    from our projects in order to build the solutions to their own problems.
</p>

<p>
    The first "standard" that came from this was <a 
    href="https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-0.md">PSR-0</a>, 
    which promoted a standard class naming convention that uses a 1:1 relationship 
    between the namespace and/or vendor prefix and the directory hierarchy, and the 
    class name and the filename in which it lives. To this day, there are both 
    those who hail this as a great step forward for cooperation, and simultaneously 
    others who feel it's a terrible practice. 
</p>

<p>
    And then nothing, for years. But a little over a year ago, there was a new 
    push by a number of folks wanting to do more. Paul Jones did a remarkable 
    job of spearheading the next <a href="https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-1-basic-coding-standard.md">two</a> 
    <a href="https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-2-coding-style-guide.md">standards</a>, 
    which centered around coding style. Again, just like with PSR-0, we had 
    both those feeling it was a huge step forward, and those who loathe the 
    direction.
</p>

<p>
    What was interesting, though, was that once we started seeing some new energy
    and momentum, it seemed that <em>everyone</em> wanted a say. And we started 
    getting dozens of folks a week asking to be voting members, and new proposal
    after new proposal. Whether or not somebody likes an existing standard, they
    want to have backing for a standard they propose.
</p>

<p>
    And this is when we started seeing proposals surface for shared interfaces, first
    around caching, and now around logging (though the latter is the first up for
    vote).
</p><h2>Shared Interfaces</h2>

<p>
    The idea around shared interfaces is simple: if we can come to a consensus on
    the basic interface for a common application task, libraries and frameworks
    can typehint on that shared interface, allowing developers to drop in the 
    implementation of their choosing -- or even a standard, reference implementation.
    The goal is to prevent Not Invented Here (NIH) syndrome, as well as to make
    it simpler to re-use components between one library and another. As an example,
    if you're using Framework A, and it has a caching library, and you're consuming
    ORM B, you'd be able to pass the same cache object to the ORM as you use in the
    framework.
</p>

<p>
    Great goals, really.
</p>

<p>
    But I'm not sure I buy into them.
</p>

<h2>Problems</h2>

<p>
    First, I agree that NIH is a problem.
</p>

<p>
    Second, I <em>also</em> think there's space for <em>multiple 
    implementations</em> of any given component. Often there are different 
    approaches that different authors will take: one might focus on 
    performance, another on having multiple adapters for providing different 
    capabilities, etc. Sometimes having a different background will present 
    different problem areas you want to resolve. As such, having multiple 
    implementations can be a very good thing; developers can look at what each 
    provides, and determine which solves the particular issues presented in the 
    current project.
</p>

<p>
    Because of this latter point, I have my reservations about shared interfaces.
</p>

<p>
    What if a particular approach requires deviating from the shared interface in 
    order to accomplish its goals? Additionally, in order to keep the greatest
    amount of compatibility between projects, shared interfaces tend to be so
    generic that specific implementations require developers to do a ton of manual
    type checking and munging of parameters, leading to more overhead, more difficulty
    testing and maintaining, and more difficulty documenting and understanding.
</p>

<p>
    As an example, consider the following (made up) signature for a log method:
</p>

<div class="example"><pre><code language="php">
public function log($message, array $context = null);
</code></pre></div>

<p>
    What if your library supports an idea of priorities? Where would that 
    information go in the above signature -- and would that differ between 
    libraries -- would one library use the key for a completely different 
    purpose? What about logging objects -- the signature doesn't say you can't, 
    but how would I know if a specific implementation supports it, and won't 
    blow up if I do pass one? Why must the <code>$context</code> be an array -- 
    why won't any <code>Traversable</code> or <code>ArrayAccess</code> object 
    work?
</p>

<p>
    Basically, by being overly generic, the signature becomes a liability for
    those implementing the interface; it prevents meaningful interoperability
    and leads to splintering implementations.
</p>

<p><em>
    (Please note: the above is completely fictional and has no bearing
    on current proposed or accepted standards. It is a thought exercise
    only.)
</em></p>

<p>
    Furthermore, if a given project writes their own implementation of a 
    component, and it has specialized features, why would they want to typehint
    on a generic, shared interface that doesn't implement those features? This
    would be counter-intuitive, as the project would then need to either check on
    additional interfaces for the specialized capabilities, duck-type, etc. --
    all of which make for more maintenance and code.
</p>

<p>
    In summary, my primary problem with the idea of shared interfaces is that I 
    feel there is always room for new thinking and ideas in any given problem 
    space, and that this thinking should not be restricted by what already 
    exists. Secondarily, I feel that it's okay for a given project to be 
    selective about what capabilities it requires for its internal consumption 
    and consistency, and should not limit itself to a standardized interface.
</p>

<h2>But, but, SHARING</h2>

<p>
    <em>Remember, the first point I made was that I think NIH is a 
    problem.</em> How do I reconcile that with a firm stance against shared 
    interfaces?
</p>

<p>
    Easy: <a href="http://en.wikipedia.org/wiki/Bridge_pattern">bridges</a> 
    and/or <a href="http://en.wikipedia.org/wiki/Adapter_pattern">adapters</a>.
</p>

<p>
    Let's go back to that example of Framework A, its caching library, and working
    with ORM B.
</p>

<p>
    Let's assume that ORM B defines an interface for caching, and let's say it
    looks like this:
</p>

<div class="example"><pre><code language="php">
interface CacheInterface
{
    public function set($key, $data);
    public function has($key);
    public function get($key);
}
</code></pre></div>

<p>
    Furthermore, we'll assume that the expected parameter values and return types
    are documented.
</p>

<p>
    What we as a consumer of both Framework A and ORM B can do is build an 
    <em>implementation</em> of <code>CacheInterface</code> that accepts a cache
    instance from Framework A, and proxies the various interface methods to that
    instance.
</p>

<div class="example"><pre><code language="php">
class FrameworkACache implements CacheInterface
{
    protected $cache;

    public function __construct(Cache $cache)
    {
        $this->cache = $cache;
    }

    public function set($key, $data)
    {
        $item = new CacheItem($key, $data);
        $this->cache->setItem($item);
    }

    public function has($key)
    {
        return $this->cache->exists($key);
    }

    public function get($key)
    {
        $item = $this->cache->getItem($key);
        return $item->getData();
    }
}
</code></pre></div>

<p>
    Assuming your code is well-decoupled, and you're using some sort of Inversion of
    Control container, you can likely create a factory for your ORM that will grab
    the above class, with the cache injected, and inject it into the ORM instance. 
    Yes, it's a bit more work, but it's difficult to question the end result: 
    shared caching between the framework and the ORM - and no need for shared 
    interfaces, nor any need to sacrifice features within the framework or the 
    ORM.
</p>

<h2>Sharing is good, developing solutions is better</h2>

<p>
    I think the core idea of the php-fig group is sound: <em>let's all start thinking
    about how we can make it easier to operate with each other</em>. That said, my 
    thoughts on how to accomplish that goal have changed significantly, and 
    boil down to:
</p>

<ul>
    <li>Use naming conventions that will reduce collisions (i.e., use 
        per-project vendor prefixes/namespaces)</li>
    <li>Use semantic versioning</li>
    <li>Keep your installation packages segregated</li>
    <li>Have a simple, discoverable way to autoload</li>
    <li>Provide interfaces for anything that could benefit from alternate implementations</li>
    <li>Don't write code that has side-effects in the global namespace 
        (including altering PHP settings or superglobals)</li>
</ul>

<p>
    Following these principals, you can play nice with each other, while still 
    fostering innovative and differentiating solutions to shared problems.
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>PHP Master Series on Day Camp For Developers</title>
      <pubDate>Tue, 18 Dec 2012 20:24:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-12-18-php-master-series.html</link>
      <guid>http://mwop.net/blog/2012-12-18-php-master-series.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    <a href="http://blog.calevans.com">Cal Evans</a> has organized another 
    DayCamp4Developers event, this time entitled "<a 
    href="http://blog.calevans.com/2012/11/19/php-master-series-vol-1">PHP 
    Master Series, Volume 1</a>". I'm honored to be an invited speaker for this 
    first edition, where I'll be presenting my talk, "Designing Beautiful Software".
</p>

<p>
    Why would you want to participate? Well, for one, because you can interact directly
    with the various speakers during the presentations. Sure, you can likely find the slide
    decks elsewhere, or possibly even recordings. But if we all do our jobs right, we'll
    likely raise more questions than answers; if you attend, you'll get a chance to ask
    some of your questions immediately, <em>and we may even answer them!</em>
</p>

<p>
    On top of that, this is a fantastic lineup of speakers, and, frankly, not a lineup 
    I've ever participated in. In a typical conference, you'd likely see one or two of
    us, and be lucky if we weren't scheduled against each other; if you attend 
    this week, you'll get to see us all, back-to-back. 
</p>

<p>
    What else will you be doing this Friday, anyways, while <a 
    href="http://en.wikipedia.org/wiki/2012_phenomenon">you wait for the end of the 
    world?</a>
</p>

<p>
    So, do yourself a favor, and <a 
    href="http://phpmasterseriesv1.eventbrite.com/">register today</a>!
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>My ZendCon Beautiful Software Talk</title>
      <pubDate>Sat, 17 Nov 2012 13:53:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-11-17-zendcon-beautiful-software.html</link>
      <guid>http://mwop.net/blog/2012-11-17-zendcon-beautiful-software.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    Once again, I spoke at <a href="http://www.zendcon.com/">ZendCon</a> 
    this year; in talking with <a href="http://twitter.com/chwenz">Christian Wenz</a>,
    we're pretty sure that the two of us and <a href="http://andigutmans.blogspot.com">Andi</a>
    are the only ones who have spoken at all eight events.
</p>

<p>
    Unusually for me, I did not speak on a Zend Framework topic, and had
    only one regular slot (I also co-presented a Design Patterns tutorial
    with my team). That slot, however, became one of my favorite talks I've
    delivered: "Designing Beautiful Software". I've given this talk a couple
    times before, but I completely rewrote it for this conference in order 
    to better convey my core message: beautiful software is maintainable
    and extensible; writing software is a craft.
</p>

<p>
    I discovered today that not only was it recorded, but it's been <a href="http://youtu.be/mQsQ6QZ4dGg">posted
    on YouTube</a>:
</p>

<iframe width="420" height="315" src="http://www.youtube.com/embed/mQsQ6QZ4dGg" frameborder="0" allowfullscreen></iframe><p>
    I've also <a href="/slides/2012-10-25-BeautifulSoftware/BeautifulSoftware.html">posted the slides</a>.
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Zend Server, ZF2, and Page Caching</title>
      <pubDate>Mon, 05 Nov 2012 21:25:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-11-05-zend-server-caching.html</link>
      <guid>http://mwop.net/blog/2012-11-05-zend-server-caching.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    Zend Server has a very cool
    <a href="http://www.youtube.com/watch_v=i2XXn2SA5zM.html" target="_blank">Page Caching feature</a>. Basically, you can provide
    URLs or URL regular expressions, and tell Zend Server to provide full-page
    caching of those pages. This can provide a tremendous performance boost, without
    needing to change anything in your application structure; simply enable it for a
    set of pages, and sit back and relax.
</p><p style="text-align: center;">
    <img 
        style="max-width: 100%; max-height: 100%;"
        src="/images/blog/2012-11-04-Server-CachingRule.png"
        alt="Zend Server Page Caching"
        title="Zend Server Page Caching" />
</p>

<p>
    However, this feature is not entirely straight-forward when using a framework
    that provides its own routing, such as ZF2. The reason is because it assumes by
    default that each match maps to a specific file on the filesystem, and prepares
    the caching based on the actual <em>file</em> it hits. What this means for ZF2 and other
    similar frameworks is that any page that matches will return the cached version
    for the <em>first</em> match that also matches the same <em>file</em> -- i.e., <code>index.php</code> in
    ZF2. That's every page the framework handles. As an example, if I match on <code>/article/\d+</code>, it matches
    this to the file <code>index.php</code>, and then any other match that resolves to
    <code>index.php</code> gets served that same page. Not handy.
</p>

<p>
    The good part is that there's a way around this.
</p>

<p>
    When creating or modifying a caching rule, simply look for the text, "Create a
    separate cached page for each value of:" and click the "Add Parameter" button.
    Select <code>_SERVER</code> from the dropdown, and type <code>[REQUEST_URI]</code> for the value. Once
    saved, each page that matches the pattern will be cached separately.
</p>

<p>
    <img 
        style="max-width: 100%; max-height: 100%;"
        src="/images/blog/2012-11-04-Server-Caching-Request.png"
        alt="Zend Server Page Caching by Request"
        title="Zend Server Page Caching by Request" />
</p>

<p>
    Note: the <code>_SERVER</code> key may vary based on what environment/OS you're deployed
    in. Additionally, it may differ based on how you define rewrite rules -- some
    frameworks and CMS systems will append to the query string, for instance, in
    which case you may want to select the "entire query string" parameter instead of
    <code>_SERVER</code>; the point is, there's likely a way for you to configure it.
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>OpenShift, ZF2, and Composer</title>
      <pubDate>Thu, 01 Nov 2012 20:25:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-11-01-openshift-zf2-composer.html</link>
      <guid>http://mwop.net/blog/2012-11-01-openshift-zf2-composer.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
I was recently shopping around for inexpensive cloud hosting; I want to try out
a couple of ideas that may or may not have much traffic, but which aren't suited
for my VPS setup (the excellent <a href="http://servergrove.com/">ServerGrove</a>); additionally, I'm unsure how long
I will maintain these projects. My budget for this is quite small as a result;
I'm already paying for hosting, and am quite happy with it, so this is really
for experimental stuff.
</p>

<p>
I considered Amazon, Orchestra.io, and a few others, but was concerned about the
idea of a ~$50/month cost for something I'm uncertain about. 
</p>

<p>
When I asked in <a href="irc://irc.freenode.net/zftalk.dev">#zftalk.dev</a>, someone
suggested <a href="http://openshift.redhat.com/">OpenShift</a> as an idea, and
coincidentally, the very next day
<a href="http://www.zend.com/en/company/news/press/379_red-hat-expands-openshift-ecosystem-with-zend-partnership-to-offer-professional-grade-environment-for-php-developers">Zend announced a partnership with RedHat surrounding OpenShift</a>.  The stars were in alignment.
</p>

<p>
In the past month, in the few spare moments I've had (which included an
excellent OpenShift hackathon at ZendCon), I've created a quick application that
I've deployed and tested in OpenShift. These are my findings.
</p><h2>ZF2</h2>

<p>
    I didn't really have to do anything different to have <a 
    href="http://framework.zend.com/">zf2</a> work; the standard 
    <code>.htaccess</code> provided in the skeleton application worked 
    flawlessly the first time (I've worked with some cloud environments where 
    this is not the case).
</p>

<p>
The only frustration I had was the default directory structure OpenShift foists
upon us: 
</p>

<div class="example"><pre>
data/
libs/
misc/
php/
</pre></div>

<p>
This is not terrible, by any stretch. However, it's attempting to dictate the
application structure, which I'm not terribly happy with -- particularly as my
structure may vary based on the framework I'm using (or not!), and because I may
already have a project written that I simply want to deploy.
</p>

<p>
In particular, the <code>php</code> directory is galling -- it's simply the document root.
Most frameworks I've used or seen call the equivalent directory <code>public</code>, or
<code>web</code>, or <code>html</code> -- but never <code>php</code> (in large part because the only PHP file
under the document root in most frameworks is the <code>index.php</code> that acts as the
front controller). It would be nice if this were configurable.
</p>

<p>
This conflicts a bit with how a ZF2 app is structured. I ended up doing the
following:
</p>

<ul>
<li>
Removed <code>php</code> and symlinked my <code>public</code> directory to it.
</li>
<li>
Removed <code>libs</code> and symlinked my <code>vendor</code> directory to it.
</li>
<li>
Removed <code>misc</code> as I had no need to it.
</li>
</ul>

<p>
Nothing too big, thankfully -- but problematic from the perspective of, "I've
already developed this app, but now I have to make changes for it to work on a
specific cloud vendor."
</p>

<h2>Composer</h2>

<p>
    My next question was how to use <a 
    href="http://getcomposer.org/">Composer</a> during my deployment process, 
    and some some googling <a href="https://openshift.redhat.com/community/content/support-for-git-clone-on-the-server-aka-support-php-composerphar">found 
    some answers for me</a>.
</p>

<p>
Basically, I needed to create a <code>deploy</code> task that does two things:
</p>

<ul>
<li>
    Unset the <code>GIT_DIR</code> environment variable. Evidently, the build 
    process operates as part of a git hook, and since Composer often uses git 
    repositories, this can lead to problems.
</li>
<li>
    Change directory to <code>OPENSHIFT_REPO_DIR</code>, which is where the 
    application root (not document root!) lives.
</li>
</ul>

<p>
    Once I did those, I could run my normal composer installation. The 
    <code>deploy</code> task looks like this:
</p>

<div class="example"><pre><code language="bash">
#!/bin/bash
# .openshift/action_hooks/deploy
( unset GIT_DIR ; cd $OPENSHIFT_REPO_DIR ; /usr/local/zend/bin/php composer.phar install )
</code></pre></div>

<p>
This leads into my next topic.
</p>

<h2>Deployment</h2>

<p>
    First off, as you probably guessed from that last secton, there 
    <strong>are</strong> hooks for
    deployment -- it doesn't have to be simply git. I like this, as I may have
    additional things I want to do during deployment, such as retrieving and
    installing site-specific configuration files, installing Composer-defined
    dependencies (as already noted), etc.
</p>

<p>
    Over all, this is pretty seamless, but it's not without issues. I've been 
    told that some of my issues are being worked on, so those I won't bring up 
    here. The ones that were a bit strange, and which caught me by surprise, 
    though, were:
</p>

<ul>
<li>
    Though the build process creates the site build from git, your 
    <strong>submodules are not updated recursively</strong>. This tripped me 
    up, as I was using <a href="https://github.com/EvanDotPro/EdpMarkdown">EdpMarkdown</a>,
    and had installed it as a submodule. I ended up having to import it, and its
    own submodule, directly into my project so that it would work.
</li>
<li>
    I installed the <a href="http://www.mongodb.org/">MongoDB</a> cartridge. Ironically, it was not then enabled in
    Zend Server, and I had to go do this. This should be turnkey.
</li>
<li>
    <code>/usr/bin/php</code> is not the same as <code>/usr/local/zend/bin/php</code>. This makes no
    sense to me if I've installed Zend Server as my base gear. Considering
    they're different versions, this can be hugely misleading and lead to errors.
    I understand there are reasons to have both -- so simply be aware that if you
    use the Zend Server gear, your tasks likely should use
    <code>/usr/local/zend/bin/php</code>.
</li>
</ul>

<h2>The good parts?</h2>

<ul>
<li>
    <a href="https://openshift.redhat.com/community/faq/i-have-deployed-my-app-but-i-don%E2%80%99t-like-telling-people-to-visit-myapp-myusernamerhcloudcom-how-c">You 
    can alias an application to a DNS CNAME</a> -- meaning you can point your
    domain name to your OpenShift applications. Awesome!
</li>
<li>
    Simplicity of adding capabilities, such as Mongo, MySQL, Cron, and others. 
    In most cases, this is simply a "click on the button" and it's installed 
    and available.
</li>
<li>
    <a href="http://www.zend.com/en/products/server">Zend Server</a>. For 
    most PHP extensions, I can turn them on or off with a few
    mouse clicks. If I want page-level caching, I don't have to do anything to my
    application; I can simply setup some rules in the Zend Server interface and
    get on with it, and enjoy tremendous boosts to performance. I used to enjoy
    taming and tuning servers; most days anymore, I just want them to work.
</li>
<li>
    <a href="https://openshift.redhat.com/community/developers/remote-access">SSH</a> 
    access to the server, with a number of commands to which I've been given
    <code>sudoer</code> access. If you're going to sandbox somebody,
    this is a fantastic way to do it. Oh, also: SSH tunnels to services like Mongo
    and MySQL just work (via the <code>rhc-port-forward</code> command).
</li>
</ul>

<h2>Summary</h2>

<p>
Over all, I'm quite pleased. While it took me a bit to find the various
incantations I needed, the service is quite flexible. For my needs, considering
I'm doing experimental stuff, the price can't be beat (the current developer
preview is free). Considering most stuff I do will fall into this or the basic
tier, and that most cartridges do not end up counting against your alotment of
gears, the pricing ($0.05/hour) is extremely competitive. 
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Screencasting on Linux</title>
      <pubDate>Thu, 20 Sep 2012 22:30:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-09-20-screencasting-on-linux.html</link>
      <guid>http://mwop.net/blog/2012-09-20-screencasting-on-linux.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    I've been wanting to do screencasts on Linux for some time now, and my big
    stumbling block has been determining what tools to use.
</p>

<p>
    The <strong>tl;dr</strong>:
</p>

<ul>
    <li>
        Use <code>recordMyDesktop</code> to record video clips, but afterwards, re-encode them
        to AVI (<a href="#script">see the script I used</a>)
    </li>

    <li>
        Record audio to WAV, or convert compressed audio to WAV format afterwards.
    </li>

    <li>
        Use OpenShot to stitch clips together and layer audio and video tracks.
    </li>

    <li>
        Remember to reset the video length if you change the playback rate.
    </li>

    <li>
        Export to a Web + Vimeo profile for best results.
    </li>
</ul><h2 id="toc_1.1">Stumbling Blocks</h2>

<p>
<code>recordMyDesktop</code> is a fairly simple tool, and allows you to
record actions you're taking, and simultaneously capture audio. However, it
creates an ".ogv" (Ogg Vorbis video file) -- which is basically useless for
anybody not on Linux or FreeBSD. Additionally, I often like to record in
segments; this makes it less likely that I'll make mistakes, and, if I do, I
only need to record a small segment again, not the entire thing. <code>recordMyDesktop</code>
is only for creating screencasts, not merging them.
</p>

<p>
So, <code>recordMyDesktop</code> went into my toolbox for the purpose of recording the video
portion of my screencasts.
</p>

<p>
Which brings me to the next point: I also prefer to record the audio separately
from the screencast portion itself; this way I don't get typing sounds in the
recording, and I'm less likely to lose my train of thought as I'm speaking. 
To this end, I ended up using quite simply the "Sound Recorder" utility
(<code>gnome-sound-recorder</code>). It's not great, but with a reasonable microphone, it
gets the job done. I chose to record the audio as MP3 files.
</p>

<p>
However, this means that I now have video and audio tracks. So my toolbox needed
a utility for overlaying tracks and laying them out on a timeline independently.
</p>

<p>
I looked at a few different free tools for Linux, including <code>Avidemux</code>, <code>Cinelerra</code>,
and <code>PiTiVi</code>. <code>Avidemux</code> was not featurful enough, <code>Cinelerra</code> was too difficult to
learn (it's more of an advanced user's tool), and <code>PiTiVi</code> kept crashing on me.
So, I used the lazyweb, and tweeted a question asking what others were using --
and the unanimous response was <code>OpenShot</code> (<a href="http://www.openshotvideo.com/">http://www.openshotvideo.com/</a>).
</p>

<p>
<code>OpenShot</code> hit the sweet spot for me -- it was easy to pick up, and didn't crash.
However, I discovered problems when I exported my project to a video file. My
video, regardless of whether or not I changed the playback rate, always played
at about 2X normal speed. The audio always truncated 1 to 2 seconds before
completion.
</p>

<p>
In doing some research, I discovered:
</p>

<ul>
<li>
There are known issues with Ogg Vorbis video files. Evidently, the
   compression creates issues when re-encoding the video to another format.
</li>
<li>
Similarly, compressed audio can lead to issues such as truncation.
</li>
</ul>

<p>
Since <code>recordMyDesktop</code> doesn't allow you to select an alternate video codec, I
had to use <code>mencoder</code> to transcode it to another format. I chose AVI (Audio
Video Interleave, a video container format developed by Microsoft), as I knew it
had widespread support, using an mpeg4 codec (also widely supported). I used the
following script, found at
<a href="http://askubuntu.com/questions/17309/video-converter-ogv-to-avi-or-another-more-common-format">http://askubuntu.com/questions/17309/video-converter-ogv-to-avi-or-another-more-common-format</a>,
in order to encode my files:
</p>

<div id="script" class="example"><pre><code language="bash">
for f in *.ogv;do
newFile=${f%.*}
mencoder "$f" -o "$newFile.avi" -oac mp3lame -lameopts fast:preset=standard -ovc lavc -lavcopts vcodec=mpeg4:vbitrate=4000
done
</code></pre></div>

<p>
That solved the video issue, but I still had to solve the audio issues. I
quickly re-recorded one audio segment in Sound Recorder, and told it to use the
"Voice,Lossless (.wav type)". When I used this version of the audio, I had no
issues, other than the audio length being mis-reported within <code>OpenShot</code>. Instead
of re-recording all segments, I installed the "Sound Converter" utility (`sudo
aptitude isntall soundconverter`), and used that to convert all my MP3 files to 
WAV. Interestingly, <code>OpenShot</code> reported the audio lengths correctly this time; go
figure.
</p>

<p>
Once that was done, I was able to start stitching everything together. A few
notes, in the hopes others learn from my mistakes:
</p>

<ul>
<li>
Several times, I wanted my video to playback slower. This is very easy to do:
   right click on the clip, select "Properties", and select the "Speed" tab, and
   adjust as necessary. However, that's not all you need to do; you need to also
   re-adjust the <em>length</em> of the clip. Simply take the existing length, and
   divide it by the rate of play. As an example, if the length is 44 seconds,
   and you specify a 1/2 rate (0.5), you'd do 44 / 0.5 = 88, and set the length
   of the clip to 88s.
</li>
<li>
If you find that <code>OpenShot</code> is reporting your audio clip lengths incorrectly,
   use another tool to find the accurate length, and then set the length to
   that. I typically rounded up to the next second, as most tools were giving
   the floor value from rounding.
</li>
<li>
I chose to export using the Web + Vimeo HD profile. This worked perfectly for
   me. It created an mpeg4 file that I could preview in a browser, and then
   upload without issues. Your mileage may vary.
</li>
</ul>

<p>
Hopefully, this will serve as a reasonable guide for others foraying into
screencasts on Linux!
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
  </channel>
</rss>
